# ДЗ № 2 "Машинное обучение в продакшене", MADE, весна 2021.

## Установка

Если нет тестового окружения, создать виртуальное окружение:
```
conda create -n $env_name python=3.8
```
Активировать окружение, в котором будете проводить тестирование проекта:
```
conda activate $env_name
```
В активированном окружении установить необходимые пакеты:
```
pip install -r requirements.txt
```
## Запуск проекта

Скачайте docker-образ:
```
docker pull bulaevvi/online-inference:v1
```
Запустите скачанный образ:
```
docker run -p 8000:8000 bulaevvi/online_inference:v1
```
В отдельной командной оболочке запустите скрипт, делающий запросы серверу:
```
cd online_inference
python make_request.py
```

## Project Organization

    ├── requests
    │   ├── make_request.py    <- Скрипт, делающий запросы к серверу.
    │   └── requests.csv       <- Файл, из которого формируются запросы.
    ├── tests
    │   └── test_app.py        <- Тест для /predict.
    ├── app.py                 <- inference модели в виде rest-сервиса.
    ├── Dockerfile             <- Dockerfile-файл для сборки контейнера.
    ├── model.pkl              <- Сериализованная обученная модель.
    ├── README.md              <- README с описанием деталей проекта.
    └── requirements.txt       <- Описание зависимостей.

## Оптимизация размера docker image
Изначально я добавил rest-сервис к проекту, сделанному в ДЗ №1. Но получилась достаточно сложная структура, которую было сложно отлаживать. Docker-файл содержал примерно в два раза больше строк, чем текущая версия. Впоследствии я решил отказаться от этой идеи и пришел к простой и прозрачной структуре, которая есть сейчас: всего 8 файлов и 2 папки для удобства.
Кроме того, в соотвествии с Лекцией 4 я организовал Docker-файл по принципу часто меняющиеся команды - вниз, редко - наверх. Это позволило ускорить процессы сборки образа в разы.

## Выявленные сложности и их решение

В процессе создания проекта было обнаружено, что FastAPI некорректно загружает сериализованные модели с кастомными функциями. Детально я описывал данную проблему в дискорде:
https://discord.com/channels/681511874449244211/826373405577838602/844617060143202364
Кроме того, есть обсуждение на stackoverflow:
https://stackoverflow.com/questions/62953477/fastapi-could-not-find-model-defintion-when-run-with-uvicorn

Проблема проявлялась в том, что при стандартном вызове uvicorn ```uvicorn app:app``` выскакивала следующая ошибка:
```AttributeError: Can't get attribute 'get_num_features' on <module '__main__' from 'C:\\Users\\vova\\anaconda3\\envs\\ml_in_prod\\Scripts\\uvicorn.exe\\__main__.py'>```

Локально пофиксить проблему удалось следующим образом:
- В теле скрипта прописал используемые кастомные функции (участок кода отмечен соответствующим комментарием)
- Для страта сервера использую не стандартную команду uvicorn app:app, а вот такую:  
```python app.py``` для Win10 и ```python3 app.py``` для Linux
Поскольку я не владею Linux и Mac, проверить все случаи я не смог, поэтому вполне вероятно, что подобная ошибка может впоследствии возникнуть.


## Как я работал с Docker

Сборка образа:
```
cd online_inference
docker build -t bulaevvi/online_inference:v1 .
```
апуск образа на локальной машине:
```
docker run -p 8000:8000 bulaevvi/online_inference:v1
```
Загрузка образа в репозиторий:
```
docker push bulaevvi/online_inference:v1
```

## Самооценка

+ Назовите ветку homework2, положите код в папку online_inference 
+ Оберните inference вашей модели в rest сервис (FastAPI), должен быть endpoint /predict (3 балла)
+ Напишите тест для /predict  (3 балла) 
+ Напишите скрипт, который будет делать запросы к вашему сервису (2 балла)
+ Напишите dockerfile, соберите на его основе образ и запустите локально контейнер(docker build, docker run), внутри контейнера должен запускать сервис, написанный в предущем пункте, закоммитьте его, напишите в readme корректную команду сборки (4 балла)
+ Оптимизируйте размер docker image (3 доп балла) (опишите в readme.md что вы предприняли для сокращения размера и каких результатов удалось добиться)
+ Опубликуйте образ в https://hub.docker.com/, используя docker push (вам потребуется зарегистрироваться) (2 балла)
+ Напишите в readme корректные команды docker pull/run, которые должны привести к тому, что локально поднимется на inference ваша модель. Убедитесь, что вы можете протыкать его скриптом из пункта 3 (1 балл)
+ Проведите самооценку (1 доп балл)
+ создайте пулл-реквест и поставьте label hw2
+ Итого: 19 баллов
